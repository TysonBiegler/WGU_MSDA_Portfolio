---
title: "D212"
author: "Tyson Biegler"
sbutitle: "Student ID: 012170282"
format: html
editor: visual
---

```{r echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}

library(tidyverse)    # For data manipulation and visualization (includes dplyr, ggplot2)
library(cluster)      # For silhouette analysis to evaluate cluster quality
library(factoextra)   # Elbow plot


# Initial setup -----------------------------------------------------------
setwd('C:/Users/tyson/Documents/GitHub/WGU_MSDA_Portfolio/Data mining II - D212/Raw')


# Data acquisition --------------------------------------------------------
churn <- read_csv("churn_clean.csv")

# Data exploration --------------------------------------------------------
glimpse(churn)

#Checking for missing or duplicate values
sum(is.na(churn))
sum(duplicated(churn))

head(churn)

#summary statistics about the variables in the dataset
#summary(churn)

churn <- churn[, c("Tenure", "Email")]  #picked only 2 variables (Kamara, 2023)

# -------------------------------------------------------------------------
#scaling the data
churn <- as.data.frame(scale(churn))

glimpse(churn)


# Plots -------------------------------------------------------------------

ggplot(churn, aes(x = Tenure)) +
  geom_histogram(binwidth = 0.5, fill = "#2E9FDF", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Tenure", x = "Tenure", y = "Count")

ggplot(churn, aes(x = Email)) +
  geom_histogram(binwidth = 0.5, fill = "#2E9FDF", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Email", x = "Email", y = "Count")

ggplot(churn, aes(x = Tenure, y = Email)) +
  geom_point(color = "#2E9FDF", alpha = 0.5) +
  theme_minimal() +
  labs(title = "Scatter Plot of Tenure vs. Email", 
       x = "Tenure", 
       y = "Email")

# Export cleaned data to CSV ---------------------------------------------------
write.csv(churn, "C:/Users/tyson/Documents/GitHub/WGU_MSDA_Portfolio/Data mining II - D212/Cleaned/Task1/churn_cleaned_data.csv", row.names = FALSE)

# Create model ------------------------------------------------------------
set.seed(1234)

#scree plot to find the elbow and  best K value
fviz_nbclust(churn, kmeans, method = "wss") + # SOURCE: (statology,2022)
  labs(title = "Elbow Method for Best k")

km <- kmeans(churn, centers = 4, nstart = 25)

# k means with 4 clusters based on the elbow plot
print(km$centers)

#creating a datafram with the cluster assignments so i can plot 
final_data <- cbind(churn, cluster = km$cluster)



fviz_cluster(km, data = final_data, 
             palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#67B800"), 
             geom = "point"
)


#silhouette score -1 not good to +1 good. 
sil <- silhouette(km$cluster, dist(churn))
fviz_silhouette(sil)
```



**Part I: Research Question**

**A1:** Can we identify distinct groups of customers based on their tenure and email interactions, using K-means clustering, to better understand churn behavior?

**A2:** The goal of this analysis is to group customers into distinct clusters based on their tenure and email frequency to identify patterns that could be indicative of the likelihood of churn.

**Part II: Technique Justification**

**B1:** I will be using k-means clustering with two continuous variables to group customers into clusters based on their similarity in `Tenure` and `Email`. The k-means algorithm assigns each customer to a cluster based on euclidean distance, ensuring that the customers within the cluster are more similar to each other than to customers in another cluster. The expected outcome is to identify customers with distinct characteristics, such as long time customers with low email engagement or newer customers with high email engagement. These types of clusters could help to identify patterns linked to churn.

**B2:** One assumption to k-means clustering is that the data is appropriately scaled. K-means clustering is based on euclidean distances and without the proper scale, the contributions from each variable would be less meaningful and accurate. Because the `Tenure` variable has a wider range than `Email`, it is important, in this case, to scale this data.

**B3:** In this analysis, I used `Tidyverse` for basic data manipulation and visualizations. `Cluster` was used for cluster analysis. Specifically I used `silhouette()` to calculate the silhouette score that measures the quality of the k-means clustering. Lastly I used `factoextra()` to visualize the scree plot for finding the optimal k value, plotting the clusters themselves, and for plotting the silhouette scores.

**Part III: Data Preparation**

**C1:** One preprocessing goal is to scale the variables so that they have equal weight in the k-means clustering algorithim becuase the range on the vales is vastly different. So it is essential that these variables are properly scaled.

**C2:** In his D212 webinar, Dr Kamara suggests that two variables is enough for for this assessment (Kamara, 2023). Therefore, I have chosen to investigate `Email` and `Tenure`. Email and Tenure can both help to determine likelihood of churn. Tenure indicates customer stability while email frequency could indicate retention efforts. When analyzing these together with k-means, I can identify the distinct groups and analyze these customers' churn behavior.

C3: I picked two numeric variables, `Tenure` and `Email`, and then I scaled them using `scale()`.



```{r echo=FALSE, warning=FALSE, eval=FALSE}
# Preparing the data ------------------------------------------------------

churn <- churn[, c("Tenure", "Email")]  #picked only 2 variables (Kamara, 2023)

#scaling the data
churn <- as.data.frame(scale(churn))
```



C4: A copy of the cleaned dataset will be provided in the submission files and is named "churn_cleaned_data.csv".\
 

**Part IV: Analysis**

D.  Perform the data analysis, and report on the results by doing the following:

1.  Determine the optimal number of clusters in the data set, and describe the method used to determine this number.

2.  Provide the code used to perform the clustering analysis technique.\
 

**Part V: Data Summary and Implications**

E.  Summarize your data analysis by doing the following:

1.  Explain the quality of the clusters created.

2.  Discuss the results and implications of your clustering analysis.

3.  Discuss **one** limitation of your data analysis.

4.  Recommend a course of action for the real-world organizational situation from part A1 based on the results and implications discussed in part E2.\
 

**Part VI: Demonstration**

**F:** My panopto video link will be included in my submission files.

**G:**  Code Sources:

-   Kamara, K. (2023, March 19). *Data mining II - D212 Webinar* \[Video\]. Western Governors University. <https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=afbc9be3-7f3a-48ef-a862-afcb0118b043&query=D212>\

**H:** Sources:

-   StatQuest: K-means clustering - YouTube (<https://www.youtube.com/watch?v=4b5d3muPQmA>)
-   How to Use the Elbow Method in R to Find Optimal Clusters - statology.org (<https://www.statology.org/elbow-method-in-r/>)

