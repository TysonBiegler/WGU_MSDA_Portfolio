---
title: "Regression Notes"
author: "Tyson Biegler"
date: "`r Sys.Date()`"
output: html_document
---

Adding variables to your multiple regression model

<https://www.youtube.com/watch?v=EqP_DupX3-c>

```{r}
library(tidyverse)

head(women)
```

```{r}
#create a model
model1 <- lm(weight ~ height, data = women)

```

creating a linear model (lm) named 'model1'. weight described by height from the dataset 'women'

```{r}
summary(model1)
```

Residuals: Residuals are the distance from the data point (fitted points) to the line. Drawing a line from the point to the line is the residual.

Residuals should be normally distributed around 0.

**Coefficients:** Y-Intercept - conceptually meaningless in this model. Weight vs. Height y-intercept is a number saying 'when height is 0, weight is x.' Height can not be 0, so it's not necessary in this data set.

**Height** - 3.4500 means that when height increases by 1, weight increases by 3.4500. The p-value is exceptionally small at 37.85 1.09e-14, meaning that we reject the null hypothesis that there is no relationship between height and weight. So, this height value is statistically significant.

**F-statistic**: the p-value is small, 1.091e-14, meaning that the f-statistic is statistically significant. This means that at least one of the predictor variables is explanatory or predictive of the outcome variable. So if the F statistic is statistically significant, then that is good. If the p-value is big, then the model doesn't work.

**R\^2:** Multiple R-squared: 0.991 means that 99.1% of the change in weight can be explained by a change in height.

**Adjusted R\^2**: is really only for multiple regression.

# Predictive modeling

```{r}
#creating a new object that is a data frame containing new values
new_data <- data.frame(height = 68)

#use the predict() function to generate new outcome values
predict(model1, new_data)
```

in other words, if you have a height of 68 inches, you have a weight of 147.0833 lbs.

```{r}
#creating a new object that is a data frame containing new values
new_data <- data.frame(height = c(68, 70, 72))

#use the predict() function to generate new outcome values
#Rounding it so that there are no decimals.
round(predict(model1, new_data))
```

```{r}
model1 %>% 
  ggplot(aes(height, weight))+
  geom_point(size = 3, alpha = .5)+
  geom_smooth(method = lm, se =F)+
  theme_bw()+
  labs(title = "Weight explained by Height in Women",
       x = "Height (explanatory or independent variable",
       y = "Weight (outcome or dependent variable")
```

# Multiple Regression

```{r}
lm(Volume ~ Girth + Height, data = trees) %>% 
  summary()
```

Y intercept - What would the value be if the explainer values (Girth, and Height) were 0. There is no such thing as a tree with 0 height or 0 girth so the y intercept ((Intercept) -57.9877) is meaningless.

Girth - 4.7082 means that for every change in Girth, the volume increases by 4.7082.

similarly, as height increases the volume increases 0.3393.

the small p-value of 2.2e-16 means that at least one of the explainer variables (independant variables) can explain the outcome variable.

Adjusted R\^2 values is 0.9442 or 94.42% of the change in value can be explained by a combination of a change in Girth and Height.

## Categorical variables with two variables

```{r}

head(mpg)

lm(hwy ~ displ, data = mpg) %>% 
  summary()
  
```

Adding in a categorical variable

```{r}
mpg %>% 
  mutate(drv = fct_recode(drv, "2" = "f", "2" = "r")) %>% 
  ggplot(aes(displ, hwy, color = drv))+
  geom_point()+
  geom_smooth(method = lm, se = F)+
  theme_bw()+
  labs(title = "Highway Fule Efficiency explained by engine size",
       x= "Engine size",
       y= "Highway fuel efficiency",
       color = "Drive")
```

```{r}
mpg %>% 
  mutate(drv = fct_recode(drv, "2" = "f", "2" = "r")) %>% 
  lm(hwy ~ displ + drv, data = .) %>%
  summary()
```

## Adding more variables

{r} install.packages("palmerpenguins")

library(palmerpenguins)

------------------------------------------------------------------------

penguins %\>% ggplot(aes(bill_depth_mm, bill_length_mm))+ geom_point()+ geom_smooth(method = lm, se = F)+ theme_bw()+ labs(title = "Penguin Bill Length explained by Bill Depth", x = "Bill Depth", y = "Bill Length")

------------------------------------------------------------------------

penguins %\>% ggplot(aes(bill_depth_mm, bill_length_mm))+ geom_point(aes(color = species), alpha = 0.7)+ geom_smooth(aes(color = species),method = lm, se = F)+ theme_bw()+ labs(title = "Penguin Bill Length explained by Bill Depth", x = "Bill Depth", y = "Bill Length")

------------------------------------------------------------------------

lm(bill_length_mm \~ bill_depth_mm + species, data = penguins) %\>% summary()

------------------------------------------------------------------------

## Choosing Variables

------------------------------------------------------------------------

```{r}
install.packages("MASS")

library(MASS)


```

------------------------------------------------------------------------

base model of mpg and engine size

```{r}
lm(hwy ~ displ, data = mpg) %>% 
  summary()

```

------------------------------------------------------------------------

adding drive to show that the model is better with this added variable

```{r}
lm(hwy ~ displ + drv, data = mpg) %>% 
  summary()
```

------------------------------------------------------------------------

AKAIKE - Information criteria: scored the variables until there is a best score model.

```{r}
lm(hwy ~ ., data = mpg) %>% 
  step(direction = "backward", trace = 0) %>% 
  summary()


mpg %>% 
  ggplot(aes(hwy, drv))+
  geom_point()+
  geom_smooth(method = lm, se = F)+
  theme_bw()
```

------------------------------------------------------------------------

```{r}
#model with one explainer variable
lm(Fertility ~ Education, data = swiss) %>% 
  summary()

swiss %>% 
  ggplot(aes(Fertility, Education))+
  geom_point()+
  geom_smooth(method = lm, se = F)+
  theme_bw()+
  labs(title = "Fertility explained by Education",
       x = "Education",
       y = "Fertility")
```

------------------------------------------------------------------------

```{r}
#AKAIKE model that included 5 more explainer variable
lm(Fertility ~ ., data = swiss) %>% 
  step(direction = "backward", trace = 0) %>% 
  summary()

swiss %>% 
  ggplot(aes(Fertility, Agriculture, Education, Catholic, Infant.Mortality))+
  geom_point()+
  geom_smooth(method = lm, se = F)+
  theme_bw()
```

```{r}
#LOOKING FOR THE OUTLIERS FOR THE RESIDUALS
plot(model1)
```

# Co-linearity

chekcing if any of the explainer variables are telling the same story as another variabel and if so it might need to be removed so as to reduce the number of variables

In the following example, the disp and hp are both correlated with mpg, but they are also correlated with eachother. So one can maybe be removed.

```{r}
library(dplyr)

mtcars %>% 
  dplyr::select(mpg, disp, hp) %>% 
  cor() %>% 
  round(2)
```
